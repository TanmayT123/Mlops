import mlflow
import time

# ---------------------------------------------------
# 1️⃣ Set MLflow tracking URI and experiment name
# ---------------------------------------------------
# This tells MLflow to log to your running UI server
mlflow.set_tracking_uri("http://127.0.0.1:5000")

# Create (or use existing) experiment
mlflow.set_experiment("My_First_Experiment")

# ---------------------------------------------------
# 2️⃣ Start tracking a new run
# ---------------------------------------------------
with mlflow.start_run(run_name="simple_experiment"):
    # Log some parameters (like hyperparameters)
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_param("num_epochs", 10)
    mlflow.log_param("optimizer", "adam")

    # Simulate metric logging (accuracy improving each epoch)
    for epoch in range(1, 6):
        accuracy = 0.75 + epoch * 0.02  # fake accuracy values
        loss = 1.0 / epoch              # fake loss values

        # Log both accuracy and loss
        mlflow.log_metric("accuracy", accuracy, step=epoch)
        mlflow.log_metric("loss", loss, step=epoch)

        print(f"Epoch {epoch}: accuracy={accuracy:.3f}, loss={loss:.3f}")
        time.sleep(0.5)  # just to simulate training time

    # ---------------------------------------------------
    # 3️⃣ Log a simple output file as artifact
    # ---------------------------------------------------
    with open("output.txt", "w") as f:
        f.write("Training completed successfully.\n")
        f.write("Final Accuracy: {:.3f}\n".format(accuracy))
        f.write("Final Loss: {:.3f}\n".format(loss))

    # Save the file to MLflow
    mlflow.log_artifact("output.txt")

print("\n✅ Experiment tracking complete! Check MLflow UI at http://127.0.0.1:5000")



Experiment tracking is the process of recording and managing all the details of machine learning experiments — such as parameters, metrics, models, and output files — to make it easy to reproduce results, compare experiments, and improve model performance systematically.

In machine learning projects, we often try different algorithms, hyperparameters, and datasets. Without tracking, it becomes difficult to know which combination performed best.
MLflow provides an efficient way to automatically log and visualize this information.

Keep a record of all experiments.
Compare different runs to choose the best model.
Make results reproducible.
Share and collaborate with other team members.

python -m mlflow ui
